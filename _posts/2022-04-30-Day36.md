---
layout:            post
title:             "[Brief Reading Day 36] Graph Adversarial Self-Supervised Learning"
menutitle:         "[Brief Reading Day 36] Graph Adversarial Self-Supervised Learning"
date:              2022-04-30
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-supervised Learning
---

[Paper Link](https://proceedings.neurips.cc/paper/2021/file/7d3010c11d08cf990b7614d2c2ca9098-Paper.pdf)

This paper proposes an adversarial self-supervised learning (GASSL) framework for learning unsupervised representations of graph data without any handcrafted views. GASSL automatically generates challenging views by adding perturbations to the input and are adversarially trained with respect to the encoder. The method optimizes the min-max problem and utilizes a gradient accumulation strategy to accelerate the training process.

The proposed GASSL consists of two networks, called teacher network and student network. The encoder of the student network is the moving average of the teacher network. The teacher network has an additional MLP to avoid collapse. A graph G is processed by the encoder network and momentum encoder network, respectively, the ouputs of which are performed by the outer minimization training. It is common practice to add noise to the node features and perturbation on the hidden layer output to affect the nodes with their neighborhoods would produce a more challenging view. These two perturbations consist of the inner maximization adversarial training. 

The outer ‘min’ of is non-convex and the inner ‘max’ is non-concave. This saddle-point problem could be reliably solved with stochastic gradient descent (SGD) for outer minimization and projected gradient descent (PGD) for inner maximization.

Experimental on ten graph classification datasets show that the proposed approach is superior to state-of-the-art self-supervised learning baselines, which are competitive with supervised models.
