---
layout:            post
title:             "[Brief Reading Day 27] A Simple Framework for Contrastive Learning of Visual Representations"
menutitle:         "[Brief Reading Day 27] A Simple Framework for Contrastive Learning of Visual Representations"
date:              2022-04-21
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Paper Link](https://arxiv.org/abs/2002.05709)

[Code Link](https://github.com/google-research/simclr)

This paper introduces a simple framework for contrastive learning of visual representations called SimCLR. 

The overall framework is as follows: Two separate data augmentation operators are sampled from the same family of augmentations (t ∼ T and t′ ∼ T ) and applied to each data example to obtain two correlated views. A base encoder network f(·) and a projection head g(·) are trained to maximize agreement using a contrastive loss. After training is completed, we throw away the projection head g(·) and use encoder f (·) and representation h for downstream tasks.

It is shown that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, (3) representation learning with contrastive cross entropy loss benefits from normalized embeddings and an appropriately adjusted temperature parameter. (4) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.
