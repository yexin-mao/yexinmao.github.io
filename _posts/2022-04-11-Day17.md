---
layout:            post
title:             "[Brief Reading Day 17] An Empirical Study of Graph Contrastive Learning"
menutitle:         "[Brief Reading Day 17] An Empirical Study of Graph Contrastive Learning"
date:              2022-04-11
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2109.01116)

[Code Link](https://github.com/PyGCL/PyGCL)

This paper identifies several critical design considerations within a general GCL paradigm, including augmentation functions, contrasting modes, contrastive objectives, and negative mining strategies. The important observations are as follows:

1. Topology augmentation greatly affects model performance. Augmentation functions that produce sparser graphs generally lead to better performance.
2. Feature augmentation brings additional benefits to GCL. Compositional aug- mentation at both structure and attribute level benefits GCL the most.
3. Deterministic augmentation schemes should be accompanied by stochastic augmentation.
4. Same-scale contrasting generally performs better. Downstream tasks of different granularities favor different contrasting modes.
5. Among negative-sample-based objectives, the use of InfoNCE objective leads to consistent improvements across all settings.
6. Bootstrapping Latent and Barlow Twins losses obtain promising performance on par with their negative-sample-based objectives yet reduce the computational burden.
7. Existing negative mining techniques based on calculating embedding similarities bring limited benefit to GCL.
