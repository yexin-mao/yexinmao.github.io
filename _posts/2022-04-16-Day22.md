---
layout:            post
title:             "[Brief Reading Day 22] Exploring Simple Siamese Representation Learning"
menutitle:         "[Brief Reading Day 22] Exploring Simple Siamese Representation Learning"
date:              2022-04-16
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2011.10566)
[Code Link](https://github.com/facebookresearch/simsiam)

This paper reports surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing.

The overall architecture is as follows: Two augmented views of one image are processed by the same encoder network f (a backbone plus a projection MLP). Then a prediction MLP h is applied on one side, and a stop-gradient operation is applied on the other side. The model maximizes the similarity between both sides. It uses neither negative pairs nor a momentum encoder.

In a nutshell, the method can be thought of as “BYOL without the momentum encoder”. Unlike BYOL but like SimCLR and SwAV, the method directly shares the weights between the two branches, so it can also be thought of as “SimCLR without negative pairs”, and “SwAV without online clustering”. Interestingly, SimSiam is related to each method by removing one of its core components. Even so, SimSiam does not cause collapsing and can perform competitively.
