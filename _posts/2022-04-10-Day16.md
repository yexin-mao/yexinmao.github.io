---
layout:            post
title:             "[Brief Reading Day 16] Large-Scale Representation Learning on Graphs via Bootstrapping"
menutitle:         "[Brief Reading Day 16] Large-Scale Representation Learning on Graphs via Bootstrapping"
date:              2022-04-10
category:          Graph
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2102.06514)

[Code Link](https://github.com/Namkyeong/BGRL_Pytorch)

This paper proposes the Bootstrapped Representation Learning on Graphs (BGRL) framework. It utilizes two encoders: an online and a target network. The former one passes the embedding vectors to a predictor network, which tries to predict the embeddings from the target encoder. The loss is measured as the cosine similarity and the gradient is backpropagated only through the online network (gradient stopping mechanism). The target encoder is updated using an exponential moving average of the online encoder weights. 

BGRL effectively scales to extremely large graphs and outperforms existing methods, while using only simple graph augmentations and not requiring negative examples. Due to having time and space complexity scaling only linearly in the size of the input, BGRL avoids the performance-memory trade-off inherent to contrastive methods altogether. BGRL provides performance competitive with the best contrastive methods, while using 2-10x less memory on standard benchmarks.

Leveraging the scalability of BGRL allows making full use of the vast amounts of unlabeled data present in large graphs via semi-supervised learning. In particular, the efficient use of unlabeled data for representation learning prevents representations from overfitting to the classification task, and achieves significantly higher, state-of-the-art performance. 

