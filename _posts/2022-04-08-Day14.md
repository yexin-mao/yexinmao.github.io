---
layout:            post
title:             "[Brief Reading Day 14] Contrastive Multi-View Representation Learning on Graphs"
menutitle:         "[Brief Reading Day 14] Contrastive Multi-View Representation Learning on Graphs"
date:              2022-04-08
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2006.05582)

[Code Link](https://github.com/kavehhassani/mvgrl)

This paper introduces a self-supervised approach for learning node and graph level representations (MVGRL) by contrasting structural views of graphs. 

In the proposed model, a graph diffusion is used to generate an additional structural view of a sample graph which along with a regular view are sub-sampled and fed to two dedicated GNNs followed by a shared MLP to learn node representations. The learned features are then fed to a graph pooling layer followed by a shared MLP to learn graph representations. A discriminator contrasts node representations from one view with graph representation of another view and vice versa, and scores the agreement between representations which is used as the training signal.

There are four major findings:

1. Increasing the number of views, i.e., augmentations, to more than two views does not improve the performance and the best performance is achieved by contrasting encodings from first-order neighbors and a general graph diffusion. 
2. Contrasting node and graph encodings across views achieves better results on both tasks compared to contrasting graph-graph or multi-scale encodings.
3. A simple graph readout layer achieves better performance on both tasks compared to hierarchical graph pooling methods such as differentiable pooling (DiffPool).
4. Applying regularization (except early-stopping) or normalization layers has a negative effect on the performance.

Using these findings, MVGRL achieved new state-of-the-art in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol and outperformed strong supervised baselines in 4 out of 8 benchmarks.

