---
layout:            post
title:             "[Brief Reading Day 28] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning"
menutitle:         "[Brief Reading Day 28] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning"
date:              2022-04-22
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Paper Link](https://arxiv.org/abs/2006.07733)

[Code Link](https://github.com/deepmind/deepmind-research/tree/master/byol)

This paper introduces Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL learns its representation by predicting previous versions of its outputs, without using negative pairs.

BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. The online network is defined by a set of weights θ and is comprised of three stages: an encoder fθ, a projector gθ and a predictor qθ. The target network has the same architecture as the online network, but uses a different set of weights ξ. The target network provides the regression targets to train the online network, and its parameters ξ are an exponential moving average of the online parameters θ. Note that this predictor is only applied to online branch, making the architecture asymmetric between the online and target pipeline. 

At each training step, we perform a stochastic optimization step to minimize the loss with respect to θ only, but not ξ. At the end of training, we only keep the encoder fθ. 
