---
layout:            post
title:             "[Brief Reading Day 49] BEIT: BERT PRE-TRAINING OF IMAGE TRANSFORMERS"
menutitle:         "[Brief Reading Day 49] BEIT: BERT PRE-TRAINING OF IMAGE TRANSFORMERS"
date:              2022-08-08
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Paper Link](https://arxiv.org/abs/2106.08254)

[Code Link](https://github.com/microsoft/unilm/tree/master/beit)

This paper introduces a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. 

Before pre-training, the authors learn an “image tokenizer” via autoencoding-style reconstruction, where an image is tokenized into discrete visual tokens according to the learned vocabulary. In BEIT, the authors directly use the publicly available1 image tokenizer from DALLE. During pre-training, each image has two views, i.e., image patches, and visual tokens. The authors randomly mask some proportion of image patches by blockwise masking and replace them with a special mask embedding. The patches are then fed to a backbone vision Transformer. The pre-training task aims at predicting the visual tokens of the original image based on the encoding vectors of the corrupted image. After pre-training BEIT, the authors append a task layer upon the Transformer, and fine-tune the parameters on downstream tasks.

BEIT achieves strong fine-tuning results on downstream tasks, such as image classification, and semantic segmentation, which shows that the proposed method is critical to make BERT-like pre-training work well for image Transformers. The authors also present the intriguing property of automatically acquired knowledge about semantic regions, without using any human-annotated data.

