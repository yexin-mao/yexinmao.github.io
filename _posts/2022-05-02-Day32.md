---
layout:            post
title:             "[Brief Reading Day 32] Improved Baselines with Momentum Contrastive Learning"
menutitle:         "[Brief Reading Day 32] Improved Baselines with Momentum Contrastive Learning"
date:              2022-05-02
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Paper Link](https://arxiv.org/pdf/2003.04297)

[Code Link](https://github.com/facebookresearch/moco)

This note proposes MoCo v2 through verifying the effectiveness of two of SimCLR’s design improvements by implementing them in the MoCo framework. With simple modifications to MoCo-namely, using an MLP projection head and more data augmentation—the authors establish stronger baselines that outperform SimCLR and do not require large training batches.

Some interesing findings are as follows:

1. Using the default τ = 0.07, pre-training with the MLP head improves from 60.6% to 62.9%; switching to the optimal value for MLP (0.2), the accuracy increases to 66.2%. However, in contrast to the big leap on ImageNet, the detection gains are smaller.

2. The extra augmentation (blur) alone (i.e., no MLP) improves the MoCo baseline on ImageNet by 2.8% to 63.4% and its detection accuracy is higher than that of using the MLP alone, despite much lower linear classification accuracy (63.4% vs. 66.2%), which indicates that linear classification accu- racy is not monotonically related to transfer performance in detection.

3. MoCo v2 achieves higher accuracy on ImageNet than SimCLR under the same epochs and batch size.

4. Large batches are not necessary for good accuracy, and state-of-the-art results can be made more accessible


