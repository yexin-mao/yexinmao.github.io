---
layout:            post
title:             "[Brief Reading Day 13] Graph Contrastive Learning with Adaptive Augmentation"
menutitle:         "[Brief Reading Day 13] Graph Contrastive Learning with Adaptive Augmentation"
date:              2022-04-07
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2010.14945)

[Code Link](https://github.com/CRIPAC-DIG/GCA)

This paper proposes a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, augmentation schemes are designed based on node centrality measures to highlight important connective structures. On the node attribute level, node features are corrupted by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information.

The influence of edge centrality is measured based on centrality of two connected nodes. For the choice of the node centrality function, three centrality measures, including degree centrality, eigen- vector centrality, and PageRank centrality are used due to their simplicity and effectiveness.

On the node attribute level, noise is added to node attributes via randomly masking a fraction of dimensions with zeros in node features. It is assumed that feature dimensions frequently appearing in influential nodes should be important.

The overall framework of the proposed deep Graph Contrastive representation learning with Adaptive augmentation (GCA) model is as follows:

1. First, generate two graph views via stochastic augmentation that is adaptive to the graph structure and attributes. 
2. Then, the two graphs are fed into a shared Graph Neural Network (GNN) to learn representations. 
3. Finnaly, train the model with a contrastive objective, which pulls representations of one node together while pushing node representations away from other node representations in the two views. For any node, its embedding generated in one view is treated as the anchor, the embedding of it generated in the other view forms the positive sample, and the other embeddings in the two views are naturally regarded as negative samples.

