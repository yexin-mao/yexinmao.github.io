---
layout:            post
title:             "[Brief Reading Day 29] BYOL works even without batch statistics"
menutitle:         "[Brief Reading Day 29] BYOL works even without batch statistics"
date:              2022-04-23
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-supervised Learning
---

[Paper Link](https://arxiv.org/pdf/2010.10241)

This paper disproves the hypothesis that the use of batch statistics is a crucial ingredient for BYOL to learn useful representations and hypothesizes that the main role of BN is to make the network more robust to cases when the initialization is scaled improperly. 

Specifically, if we replace BN with a combination of group normalization, GN, and weight standardization, WS, while keeping standard initializations, BYOL achieves 73.9% top-1 accuracy.

In particular, BYOL achieves 65.7% top-1 accuracy when removing BN and changing the initialization. Moreover, BYOL achieves a competitive 73.9% top-1 accuracy by replacing BN with a normalization scheme operating element-wise.
