---
layout:            post
title:             "[Brief Reading Day 33] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
menutitle:         "[Brief Reading Day 33] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
date:              2022-04-27
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-supervised Learning
---

[Paper Link](https://arxiv.org/pdf/2105.04906)

[Code Link](https://github.com/facebookresearch/vicreg)


This paper introduces VICReg(Variance-Invariance-Covariance Regularization), a simple approach to self-supervised learning based on a triple objective: learning invariance to different views with a invariance term, avoiding collapse of the representations with a variance preservation term, and maximizing the information content of the representation with a covariance regularization term. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc.

The basic idea is to use a loss function with three terms:

(1) Invariance: the mean square distance between the embedding vectors.
(2) Variance: a hinge loss to maintain the standard deviation (over a batch) of each variable of the embedding above a given threshold. This term forces the embedding vectors of samples within a batch to be different.
(3) Covariance: a term that attracts the covariances (over a batch) between every pair of (centered) embedding variables towards zero. This term decorrelates the variables of each embedding and prevents an informational collapse in which the variables would vary together or be highly correlated.

In addition, the variance regularization term stabilizes the training of other methods and leads to performance improvements.

VICReg achieves results on par with the state of the art on many downstream tasks, but is not subject to the same limitations as most other methods, particularly because it does not require the embedding branches to be identical or even similar.
