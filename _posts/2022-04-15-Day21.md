---
layout:            post
title:             "[Brief Reading Day 21] SelfGNN: Self-supervised Graph Neural Networks without explicit negative sampling"
menutitle:         "[Brief Reading Day 21] SelfGNN: Self-supervised Graph Neural Networks without explicit negative sampling"
date:              2022-04-15
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2103.14958)
[Code Link](https://github.com/zekarias-tilahun/SelfGNN)

This paper proposes SelfGNN, a novel contrastive self-supervised graph neural network (GNN) without relying on explicit contrastive terms. 

Two families of data augmentation techniques are proposed based on topology and features: Personalized PageRank, heat kernel, and Katz-index on topology, split, standardize, local degree profile, paste on features. 

The proposed architecture mimics a Siamese network that is commonly used in recent contrastive self-supervised models for representation learning. It has two parallel networks, referred to as a student and teacher networks. The first component of both networks is similar, they both use stacked GNN encoder functions, producing the representation of the input graph, which is what we use for downstream graph analytic tasks. The second block in most Siamese networks is a projection head. The authors empirically found out that for GRL it adds no qualitative gain, therefore, they drop the projection head. 

One key difference between the student and the teacher network is that the student network applies a prediction block, a MLP over the output of the representation block. The prediction block act as the brain of the student who is trying to learn what is produced by the teacher network. The second key difference between the two networks is that, the parameters of the student network are updated through a back-propagation of gradients. Whereas, the parameters of the teacher network are updated using an exponential moving average (EMA) of the parameters of the student network.

It has been analytically proven that the prediction block and the BatchNorm introduce implicit negative samples. Thus, similar to those that explicitly sample contrastive terms either from the batch or entire dataset, the implicit contrastive terms prevents the model from converging to the trivial solution of collapsing into a constant point.

Extensive experiments are carried out using seven real-world datasets and show that SelfGNN achieves a comparable performance with supervised GNNs, while performing significantly better than semi-supervised and self-supervised methods.
