---
layout:            post
title:             "[Brief Reading Day 33] GraphVICRegHSIC: Towards improved self-supervised representation learning for graphs with a hyrbid loss function"
menutitle:         "[Brief Reading Day 33] GraphVICRegHSIC: Towards improved self-supervised representation learning for graphs with a hyrbid loss function"
date:              2022-05-03
category:          Graph
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-supervised Learning
---

[Paper Link](https://arxiv.org/abs/2105.12247)

This paper proposes a hybrid loss function combining the advantages of VICReg and HSIC called VICRegHSIC. The author compares it with five different loss functions namely InfoNCE, SimCLR, Barlow Twins Loss, HSIC Loss and VICReg Loss in the Graph based SSL paradigm and has compared their performances on node classification tasks corresponding to 7 different datasets. Results show that our proposed loss function performs better than the state-of-the-art contemporaries in the framework of Graph SSL. 

The impact of several data augmentation strategies such as node dropping and attribute masking is also analyzed, showing that considering all the augmentations together gives the best results in terms of generalizability and robustness in the proposed framework. 

Ablation studies are done on batch sizes and projector dimensions showing that increasing projection dimension increases the performance of the algorithms
The possibilities of different p values for the Lp-norm in the invariance term of the proposed loss function are explored and it is shown that p = 2 works the best in most of the datasets. The values of λ and µ are also explored where it is necessarily to stick to the scenaria that λ = µ, however, such equality between the two hyperparameters seems to work best for the proposed algorithm in most cases.
