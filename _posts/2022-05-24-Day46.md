---
layout:            post
title:             "[Brief Reading Day 46] Large-scale Video Classification with Convolutional Neural Networks"
menutitle:         "[Brief Reading Day 46] Large-scale Video Classification with Convolutional Neural Networks"
date:              2022-05-24
category:          Video Understanding
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Video Understanding
---

[Paper Link](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf)

[Code Link](https://github.com/gtoderici/sports-1m-dataset/)

This paper provides an extensive empirical evaluation of CNNs on largescale video classification using a new dataset Sports-1M of 1 million YouTube videos belonging to 487 classes. The authors study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training.

The explored approaches for fusing information over temporal dimension through the network are as follows:
1. Single frame: A single-frame baseline architecture is used to understand the contribution of static appearance to the classification accuracy.
2. Early Fusion: The Early Fusion extension combines information across an entire time window immediately on the pixel level
3. Late fusion: The Late Fusion model places two separate single-frame networks with shared parameters a distance of 15 frames apart and then merges the two streams in the first fully connected layer.
4. Slow fusion: The Slow Fusion model is a balanced mix between early fusion and late fusion that slowly fuses temporal information throughout the network such that higher layers get access to progressively more global information in both spatial and temporal dimensions.

The proposed multiresolution architecture aims to strike a compromise between speed and performance by having two separate streams of processing over two spatial resolutions. 

The best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). The authors further study the generalization performance of the best model by retraining the top layers on the UCF101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).
