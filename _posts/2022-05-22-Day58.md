---
layout:            post
title:             "[Brief Reading Day 58] Two-Stream Convolutional Networks for Action Recognition in Videos"
menutitle:         "[Brief Reading Day 58] Two-Stream Convolutional Networks for Action Recognition in Videos"
date:              2022-05-22
category:          Video Understanding
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Video Understanding
---

[Paper Link](https://arxiv.org/abs/1406.2199)

[Code Link](https://github.com/woodfrog/ActionRecognition)

This paper proposes a two-stream ConvNet architecture which incorporates spatial and temporal networks. The ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Also, multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both.

The architectures of spatial and temporal networks are two networks similar to AlexNet with different inputs. The spatial stream ConvNet operates on individual video frames, effectively performing action recognition from still images. The input to temporal network is formed by stacking optical flow displacement fields between several consecutive frames. Such input explicitly describes the motion between video frames, which makes the recognition easier, as the network does not need to estimate motion implicitly. Softmax scores of these two networks are combined by late fusion.

In multitask learning, a ConvNet architecture is modified so that it has two softmax classification layers on top of the last fully connected layer: one softmax layer computes HMDB-51 classification scores, the other one – the UCF-101 scores. Each of the layers is equipped with its own loss function, which operates only on the videos, coming from the respective dataset. The overall training loss is computed as the sum of the individual tasks’ losses, and the network weight derivatives can be found by back-propagation.



