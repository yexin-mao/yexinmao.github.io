---
layout:            post
title:             "[Brief Reading Day 28] Understanding Self-Supervised and Contrastive Learning with \"Bootstrap Your Own Latent\" (BYOL)"
menutitle:         "[Brief Reading Day 28] Understanding Self-Supervised and Contrastive Learning with \"Bootstrap Your Own Latent\" (BYOL)"
date:              2022-04-26
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Blog Link](https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)

[Code Link](https://github.com/untitled-ai/self_supervised)

This blog highlights two surprising findings from the work on reproducing BYOL:

(1) BYOL often performs no better than random when batch normalization is removed, and

(2) the presence of batch normalization implicitly causes a form of contrastive learning.

Batch normalization is critical in BYOL to avoid mode collapse as mode collapse is prevented precisely because all samples in the mini-batch cannot take on the same value after batch normalization. One way to prevent mode collapse is to identify the common mode between examples. Batch normalization identifies this common mode between examples of a mini-batch and removes it by using the other representations in the mini-batch as implicit negative examples. We can thus view batch normalization as a novel way of implementing contrastive learning on embedded representations.

with batch normalization, BYOL learns by asking, "how is this image different from the average image?". The explicit contrastive approach used by SimCLR and MoCo learns by asking, "what distinguishes these two specific images from each other?" These two approaches seem equivalent, since comparing an image with many other images has the same effect as comparing it to the average of the other images. Whether contrasting different images with each other or contrasting each image with the average of all images, a major part of learning is understanding how things differ from each other.

Also, earlier batch normalization layers eventually have the same effect and Removing all batch normalization completely prevents learning unless at least one technique is used to prevent mode collapse.
