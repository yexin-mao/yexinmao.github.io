---
layout:            post
title:             "[Brief Reading Day 54] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
menutitle:         "[Brief Reading Day 54] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
date:              2022-05-18
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Natural Language Processing
---

[Paper Link](https://arxiv.org/pdf/1810.04805)

[Code Link](https://github.com/google-research/bert)

This paper introduces a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.

For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B

We pre-train BERT using two unsupervised tasks: Masked LM and Next Sentence Prediction (NSP). Details can be found in paper.

The pre-training procedure largely follows the existing literature on language model pre-training. For finetuning, BERT uses the self-attention mechanism to unify downstream tasksâ€”whether they involve single text or text pairs, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as en- tailment or sentiment analysis.
