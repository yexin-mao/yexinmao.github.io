---
layout:            post
title:             "[Brief Reading Day 39] Attention Is All You Need"
menutitle:         "[Brief Reading Day 39] Attention Is All You Need"
date:              2022-05-17
category:          Natural Language Processing
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Natural Language Processing
---

[Paper Link](https://arxiv.org/abs/1706.03762)

[Code Link](https://github.com/jadore801120/attention-is-all-you-need-pytorch)

This is a classic paper, whcih proposes a simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 

The Transformer has an encoder-decoder structure. The encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. The transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.

An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Multi-head attention is used, allowing the model to jointly attend to information from different representation subspaces at different positions.

For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the authors achieved a new state of the art at that time. In the former task their best model outperformed even all previously reported ensembles.
