---
layout:            post
title:             "[Brief Reading Day 43] Augmentation-Free Self-Supervised Learning on Graphs"
menutitle:         "[Brief Reading Day 43] Augmentation-Free Self-Supervised Learning on Graphs"
date:              2022-05-07
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-supervised Learning
---

[Paper Link](https://arxiv.org/abs/2112.02472)

[Code Link](https://github.com/Namkyeong/AFGRL)

This paper proposes a novel augmentation-free self-supervised learning framework for graphs, named AFGRL, which requires neither augmentation techniques nor negative samples for learning representations of graphs.

Precisely, instead of creating two arbitrarily augmented views of a graph and expecting them to preserve the semantics of the original graph, the authors use the original graph per se as one view, and generate another view by discovering, for each node in the original graph, nodes that can serve as positive samples via k-nearest-neighbor (k-NN) search in the representation space. Then, given the two semantically related views, they aim to predict, for each node in the first view, the latent representations of its positive nodes in the second view. However, naively selecting positive samples based on k-NN search to generate an alternative view can still alter the semantics of the original graph. Hence, they introduce a mechanism to filter out false positives from the samples discovered by k-NN search. In a nutshell, they consider a sample to be positive only if either 1) it is a neighboring node of the target node in the adjacency matrix (local perspective), capturing the relational inductive bias inherent in the graph-structured data, or 2) it belongs to the same cluster as the target node (global perspective).

The major benefit of AFGRL over other self-supervised methods on graphs is its stability over hyperparameters while maintaining competitive performance even without using negative samples for the model training, which makes AFGRL practical. Through experiments on multiple graphs on various downstream tasks, it empirically shows that AFGRL is superior to the state-of-the-art methods that are sensitive to augmentation hyperparameters.
