---
layout:            post
title:             "[Brief Reading Day 8] Understanding Dimensional Collapse in Contrastive Self-supervised Learning"
menutitle:         "[Brief Reading Day 8] Understanding Dimensional Collapse in Contrastive Self-supervised Learning"
date:              2022-04-02
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Contrastive Learning, Dimensional Collapse
---

[Paper Link](https://arxiv.org/abs/2110.09348)
[Code Link](https://github.com/facebookresearch/directclr)

This paper shows that contrastive learning suffers from dimentional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. In this paper, the authors shed light on the dynamics at play in contrastive learning that leads to dimensional collapse and propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector.

There are two mechanisms causing the dimensional collapse in contrastive learning: 

1.Strong augmentation along feature dimensions 

2.Implicit regularization driving models toward low-rank solutions.

In contrast to all recent state-of-the-art self-supervised learning methods where a projector is substantially added to improves the quality of the learned representation and downstream performance, this paper proposes to remove the projector in contrastive learning by directly sending a subvector of the representation vector to the loss function and optimizing the representation space. DirectCLR picks a subvector of the representation z = r[0 : d0], where d0 is a hyperparameter. Then, it applies a standard InfoNCE loss on this normalized subvector. 

DirectCLR outperforms SimCLR with a linear projector on ImageNet. Theory and experiment supports can be found in the paper.


