---
layout:            post
title:             "[Brief Reading Day 18] Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph
Representation Learning"
menutitle:         "[Brief Reading Day 18] Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph
Representation Learning"
date:              2022-04-12
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2105.05682)

[Code Link](https://github.com/GRAND-Lab/MERIT)

This paper proposes a novel self-supervised approach to learn node representations by enhancing Siamese self-distillation with multi-scale contrastive learning. This paper is the first to use the Siamese networks on node representation learning. Two types of contrastive objectives for self-supervised node representation learning are proposed based on Siamese networks, which regularizes each other and are capable of providing a more effective graph encoder.

This model mainly consists of three components: Graph augmentations, cross-network contrastive learning, and cross-view contrastive learning. The authors apply graph diffusion, edge modification, subsampling and node feature masking as graph augmentation methods. Through graph augmentations, two graph views are constructed, based on which an online network and a target network are employed to generate node representations for each view. This is a Siamese architecture, which consists of two identical encoders with an extra predictor on top of the online encoder. Then, a multi-scale contrastive learning scheme, which utilizes both cross-network and cross-view contrastive modules, is deployed to learn effective node embeddings.

Extensive experiments are conducted on various real-world datasets and the effectiveness of the proposed method is validated over state-of-the-art methods in self-supervised graph representation learning.
