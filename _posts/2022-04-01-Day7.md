---
layout:            post
title:             "[Brief Reading Day 7] Adversarial Graph Augmentation to Improve Graph Contrastive Learning"
menutitle:         "[Brief Reading Day 7] Adversarial Graph Augmentation to Improve Graph Contrastive Learning"
date:              2022-04-01
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2106.05819)

[Paper Link](https://github.com/susheels/adgcl)

GNNs trained by traditional GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks. This paper proposes a novel principle, termed adversarial-GCL (AD-GCL), which enables GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL. A practical instantiation based on trainable edge-dropping graph augmentation is also designed. 

The key componet in GCL is InfoMax principle, however, reasearchers have found that InfoMax principle may be risky because it may push encoders to capture redundant information that is irrelevant to the downstream tasks. This observation reminds us of another principle, termed information bottleneck (IB). InfoMax asks for maximizing the information from the original graph, while GIB asks for minimizing such information but simultaneously maximizing the information that is relevant to the downstream tasks. Unfortunately, GIB requires the knowledge of the class labels Y from the downstream task and thus does not apply to self-supervised training of GNNs where there are few or no labels. 

To address this, the authors develop a GCL approach that uses adversarial learning to avoid capturing redundant information during the representation learning. In general, GCL methods use graph data augmentation (GDA) processes to perturb the original observed graphs and decrease the amount of information they encode. Then, the methods apply InfoMax over perturbed graph pairs (using different GDAs) to train an encoder f to capture the remaining information.

AD-GCL contains two components for graph data encoding and graph data augmentation. The GNN encoder maximizes the mutual information between the original graph and the augmented graph while the GNN augmenter optimizes the augmentation to remove the information from the original graph. The instantiation of AD-GCL proposed in this work uses edge dropping: An edge of the graph is randomly dropped according to Bernoulli(ωe), where ωe is parameterized by the GNN augmenter.

AD-GCL2 is validated by comparing with the state-of-the-art GCL methods and achieve performance gains in unsupervised, transfer, and semi-supervised learning settings overall with 18 different benchmark datasets for the tasks of molecule property regression and classification, and social network classification.
