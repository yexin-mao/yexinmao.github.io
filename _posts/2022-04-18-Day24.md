---
layout:            post
title:             "[Brief Reading Day 24] Graph-MLP: Node Classification without Message Passing in Graph"
menutitle:         "[Brief Reading Day 24] Graph-MLP: Node Classification without Message Passing in Graph"
date:              2022-04-18
category:          Graph
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2106.04051)
[Code Link](https://github.com/yanghu819/Graph-MLP)

This paper proposes a pure multilayer-perceptron-based framework, Graph-MLP with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. 

The MLP-based structure consists of linear layers followed by activation, normalization, and dropout. More specifically, a linear-activation-layer normalization-dropout block is utilized and two following additional linear layers serve as the prediction heads. For activation, the authors use Gelu. Layernorm is applied instead of batch normalization for the training stability. Dropout is to avoid over-fitting. The second last linear layer is supervised for feature transformation with the proposed NContrast loss. In NContrast loss, for each node, its r-hop neighbors are regarded as the positive samples, while the other nodes are sampled as the negative ones. The loss encourages positive samples to be closer to the target node and pushes negative samples away from the target one in terms of feature distance. The last linear layer is guided for learning node classification with a traditional cross-entropy.

This design allows the model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, the framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.
