---
layout:            post
title:             "[Brief Reading Day 36] FastGCL: Fast Self-Supervised Learning on Graphs via Contrastive Neighborhood Aggregation"
menutitle:         "[Brief Reading Day 36] FastGCL: Fast Self-Supervised Learning on Graphs via Contrastive Neighborhood Aggregation"
date:              2022-05-06
category:          Graph
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-supervised Learning
---

[Paper Link](https://arxiv.org/pdf/2205.00905)

This paper proposes a simple yet effective method named FastGCL by arguing that a better contrastive schemeshould be tailored to the characteristics of graph neural networks (e.g., neighborhood aggregation). Specifically, by constructing weighted aggregated and non-aggregated neighborhood information as positive and negative samples respectively, FastGCL identifies the potential semantic information of data without disturbing the graph topology and node attributes, resulting in faster training and convergence speeds.

The overview of FastGCL is as follows: It mainly consists of a shared GNN encoder and a learnable edge-weighting module. The edge-weighting module consists of a MLP and and element-wise product process. It takes the node representation of the original graph G as input to generate the weights of the corresponding edges, then these weights are performed a elementwise product with the adjacency matrix of the original graph. The positive graph is generated by the edge-weighting module, while the graph with only self-loop (mathematically, A = I) is straightforwardly considered as the negative graph. All learnable parameters are optimized via contrastive loss.

Extensive experiments demonstrate that FastGCL has an advantage in training time and can achieve competitive performance in downstream tasks.
