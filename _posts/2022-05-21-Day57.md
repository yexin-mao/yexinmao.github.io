---
layout:            post
title:             "[Brief Reading Day 57] Masked Autoencoders Are Scalable Vision Learners"
menutitle:         "[Brief Reading Day 57] Masked Autoencoders Are Scalable Vision Learners"
date:              2022-05-21
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Visual Transformer
---

[Paper Link](https://arxiv.org/abs/2111.06377)

[Code Link]([https://github.com/google-research/vision_transformer](https://github.com/facebookresearch/mae))

This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. MAE enables us to train large models efficiently and effectively: the training is accelerated by 3× or more and the accuracy is improved. This scalable approach also allows for learning high-capacity models that generalize well and transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.

The overall architecture is as follows: A large random subset of image patches (e.g., 75%) is first masked out during pre-training. The encoder (ViT) is applied to the small subset of visible patches. Mask tokens are introduced after the encoder, and the full set of encoded patches and mask tokens is processed by a small decoder (flexibly designed in a manner that is independent of the encoder design) that reconstructs the original image in pixels. The loss function computes the mean squared error (MSE) between the reconstructed and original images in the pixel space. After pre-training, the decoder is discarded and the encoder is applied to uncorrupted images (full sets of patches) for recognition tasks.

Two observations can be taken away. First, an autoencoder—a simple self-supervised method similar to techniques in NLP provides scalable benefits on ImageNet and in transfer learning so that self-supervised learning in vision may now be embarking on a similar trajectory as in NLP. Second, MAE infers complex, holistic reconstructions, suggesting it has learned numerous visual concepts, i.e., semantics, which is hypothesized that this behavior occurs
by way of a rich hidden representation inside the MAE.

