---
layout:            post
title:             "[Brief Reading Day 20] GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training"
menutitle:         "[Brief Reading Day 20] GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training"
date:              2022-04-14
category:          Graph
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2006.09963)

[Code Link](https://github.com/THUDM/GCC)

This paper proposes a self-supervised graph neural network pre-training framework called Graph Contrastive Coding (GCC) to capture the universal network topological properties across multiple networks. The authors design GCC’s pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations.

To instantiate each component in GCC, we need to design the following three aspects:

1. Design (subgraph) instances in graphs: To address this issue, this paper proposes to use subgraphs as contrastive instances by extending each single vertex to its local structure. Specifically, for a certain vertex v, an instance is defined to be its r-ego network.
2. Define (dis)similar instances: The data augmentation is defined as graph sampling, which follows the three steps—random walks with restart (RWR), subgraph induction, and anonymization. The procedure is repaeted twice to create two data augmentations, which form a similar instance pair. If two subgraphs are augmented from different r-ego networks, they are treated as a dissimilar pair.
3. Define graph encoders: Technically, any graph neural networks can be used here as the encoder, and the GCC model is not sensitive to different choices. In practice, the authors adopt the Graph Isomorphism Network (GIN), as our graph encoder. As most GNN models require vertex features/attributes as input, the authors propose to leverage the graph structure of each sampled subgraph to initialize vertex features, which is defined as the generalized positional embedding.

The pre-trained graph neural network achieves competitive performance to its supervised trained-from-scratch counterparts in three graph learning tasks on ten graph datasets. 
