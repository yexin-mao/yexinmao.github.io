---
layout:            post
title:             "[Brief Reading Day 55] BERT for Joint Intent Classification and Slot Filling"
menutitle:         "[Brief Reading Day 55] BERT for Joint Intent Classification and Slot Filling"
date:              2022-05-19
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Natural Language Processing
---

[Paper Link](https://arxiv.org/abs/1902.10909)

[Code Link](https://github.com/monologg/JointBERT)


This paper proposes a joint intent classification and slot filling model based on BERT

BERT can be easily extended to a joint intent classification and slot filling model. Based on the hidden state of the first special token ([CLS]), denoted h1, the intent is predicted through a linear layer followed by softmax. For slot filling, the final hidden states of other tokens h2, . . . , hT are fed into a softmax layer to classify over the slot filling labels. To make this procedure compatible with the WordPiece tokenization, each tokenized input word is fed into a WordPiece tokenizer and use the hidden state corresponding to the first sub-token as input to the softmax classifier. The objective jointly models intent classification and slot filling.


Experimental results demonstrate that the proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.
