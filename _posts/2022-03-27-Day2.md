---
layout:            post
title:             "[Brief Reading Day 2] Bayesian Graph Contrastive Learning"
menutitle:         "[Brief Reading Day 2] Bayesian Graph Contrastive Learning"
date:              2022-03-27
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---



[Paper Link](https://arxiv.org/abs/2112.07823)

Existing graph contrastive learning methods are incapable of uncertainty quantification for node representations or their downstream tasks, limiting their application in high-stakes domains. This paper proposes a novel Bayesian perspective of graph contrastive learning methods. First, It introduces a generalized augmentation model and show that how the stochasticity of augmentations can be perceived as randomness in the parameters of each layer of the encoders. Second, it shows that the graph contrastive loss is equivalent to the ELBO of a variational autoencoder with stochastic function as encoder and specific choices of prior, posterior, and likelihood. As a result, the Bayesian graph contrastive learning method represents each node by a distribution in the latent space in contrast to existing techniques which embed each node to a deterministic vector. Hence, BGCL provides predictive uncertainty in downstream graph analytics tasks. Furthermore, the authors develope a Bayesian framework to infer the augmentation drop rates in each view of the contrastive model, eliminating the need for a computationally expensive search for hyperparameter tuning.

The main limitation of the proposed method is computational and space complexity of sampling generalized augmentations. To alleviate this issue, the authors use a single sample for a block of features as opposed to drawing a new sample for every input-output feature pairs. They use 8 blocks of feature, which decreases the training time substantially. Each training epoch of BGCL on Cora dataset takes 0.884 seconds compared to GRACE which takes 0.016 seconds.

