---
layout:            post
title:             "[Brief Reading Day 12] Graph Contrastive Learning Automated"
menutitle:         "[Brief Reading Day 12] Graph Contrastive Learning Automated"
date:              2022-04-06
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2103.03230)

[Code Link](https://github.com/facebookresearch/barlowtwins)

This paper proposes an objective function called BARLOW TWINS that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors.

BARLOW TWINS’s objective function can be understood through the lens of information theory, and specifically as an instanciation of the Information Bottle- neck (IB) objective. The IB objective consists in finding a representation that conserves as much information about the sample as possible while being the least possible informative about the specific distortions applied to that sample.

Both BARLOW TWINS’ and INFONCE’s objective functions have two terms, the first aiming at making the embeddings invariant to the distortions fed to the twin networks, the second aiming at maximizing the variability of the embedding learned. However, BARLOW TWINS is robust to small batches unlike the popular INFONCE-based method and benefits from using very large dimensional embeddings, unlike INFONCE-based methods which do not see a benefit in increasing the dimensionality of the output. 

More detals can be found in this paper, I think this paper is very helpful to my current research.






