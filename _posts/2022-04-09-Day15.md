---
layout:            post
title:             "[Brief Reading Day 15] InfoGCL: Information-Aware Graph Contrastive Learning"
menutitle:         "[Brief Reading Day 15] InfoGCL: Information-Aware Graph Contrastive Learning"
date:              2022-04-09
category:          Blogs
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Graph Contrastive Learning
---

[Paper Link](https://arxiv.org/abs/2110.15438)

This paper proposes InfoGCL, an information-aware graph contrastive learning framework for graph contrastive learning. This method decouples the typical contrastive learning approaches into three sequential modules and provides the theoretical analysis for reaching the optimality. In particular, the authors attempt to answer the following questions for graph contrastive learning: (i) What is the optimal augmented views? (ii) What is the optimal view encoder? (iii) What is the optimal contrastive mode?

For the optimal graph views, the amount of information shared between them is minimized, while the two views contain the same amount of information with respect to the label, which is also the amount of information that the input gprah contains about the task. For the optimal view encoder, the amount of information shared between the optimal view and the extracted representation is minimized, while the information shared between the two optimal views is kept after the encoding process of one view. The optimal contrastive mode keeps the most task-relevant information after the representations are aggregated. The role of negative samples in graph contrastive learning is also explored that negative samples are not necessarily required.

The observations in the evluation part should be considered further more.


