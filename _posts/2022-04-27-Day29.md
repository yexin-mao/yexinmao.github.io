---
layout:            post
title:             "[Brief Reading Day 29] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
menutitle:         "[Brief Reading Day 29] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
date:              2022-04-27
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Paper Link](https://arxiv.org/abs/2105.04906)

[Code Link](https://github.com/facebookresearch/vicreg)

This papers introduces VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables.

The definitions of terms in VICReg are as follows:

Invariance: the mean square distance between the embedding vectors.

Variance: a hinge loss to maintain the standard deviation (over a batch) of each variable of the embedding above a given threshold. This term forces the embedding vectors of samples within a batch to be different.

Covariance: a term that attracts the covariances (over a batch) between every pair of (centered) embedding variables towards zero. This term decorrelates the variables of each embedding and prevents an informational collapse in which the variables would vary together or be highly correlated.

VICReg achieves results on par with the state of the art on many downstream tasks, but is not subject to the same limitations as most other methods, particularly because it does not require the embedding branches to be identical or even similar.
