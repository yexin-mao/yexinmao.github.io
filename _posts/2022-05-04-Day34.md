---
layout:            post
title:             "[Brief Reading Day 34] A Note on Connecting Barlow Twins with Negative-Sample-Free Contrastive Learning"
menutitle:         "[Brief Reading Day 34] A Note on Connecting Barlow Twins with Negative-Sample-Free Contrastive Learning"
date:              2022-05-04
category:          Self-Supervised Learning
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Self-Supervised Learning
---

[Paper Link](https://arxiv.org/pdf/2104.13712)

[Code Link](https://github.com/yaohungt/Barlow-Twins-HSIC)

The original paper of Barlow Twins is motivated by the Information Bottleneck (IB) theory, assuming the representations are Gaussian and that we need to minimize the determinant of the feature cross-correlation. This note provides an alternative interpretation of the Barlow Twins’ objective by viewing it as a negative-sample-free contrastive learning objective and relates the it to Hilbert-Schmidt Independence Criterion (HSIC) maximization between augmented views, where HSIC is a contrastive approach that avoids the need of constructing negative pairs. 

HSIC resembles the original Barlow Twins’ objective with the difference on the off-diagonal terms: Barlow Twins encourages the off-diagonal terms to be zero and the HSICSSL encourages the off-diagonal terms to be negative one.

This new interpretation entails no assumption on the Gaussianity, and is thus more consistent with how Barlow Twins is used in practice. While the HSIC interpretation calls for a slight change to the original Barlow Twins’ objective, it empirically shows that the change leads to no loss in performance.



