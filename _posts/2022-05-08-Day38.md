---
layout:            post
title:             "[Brief Reading Day 38] ML-Decoder: Scalable and Versatile Classification Head"
menutitle:         "[Brief Reading Day 38] ML-Decoder: Scalable and Versatile Classification Head"
date:              2022-05-08
category:          Multi-Label Classification
author:            yexinmao
cover:             /assets/mountain-alternative-cover.jpg
tags:              Multi-label classification
---

[Paper Link](https://arxiv.org/pdf/2111.12933v2)

[Code Link](https://github.com/Alibaba-MIIL/ML_Decoder)

This paper introduces ML-Decoder, a new attention based classification head. ML-Decoder predicts the existence of class labels via queries, and enables better utilization of spatial data compared to global average pooling. By redesigning the decoder architecture, and using a novel group-decoding scheme, ML-Decoder is highly efficient, and can scale well to thousands of classes. Compared to using a larger backbone, ML-Decoder consistently provides a better speed-accuracy trade-off. ML-Decoder is also versatile - it can be used as a drop-in replacement for various classification heads, and generalize to unseen
classes when operated with word queries. Novel query augmentations further improve its generalization ability.

Self-attention removal: It is observed that during inference, the self-attention module of transformerdecoder provides a fixed transformation on the input
queries. However, as the queries are entering the crossattention module, they are subjected to a projection layer, before going through the attention operation. In practice, the projection layer can transform the queries to any desired output, making the self-attention module redundant. Hence, the self-attention layer can be removed, while still maintaining the same expressivity of the classification head, and without degrading the results.

Group-decoding: In an extreme classification scenario, even linear dependency of the classification head with the number of classes can be costly. Group-decoding is to break this coupling, and make the cross-attention module, and the feed-forward layer after it, independent of the number of classes, same as GAP operation

Extensive experimental analysis shows that ML-Decoder outperforms GAP-based heads on several classification tasks, such as multi-label, single-label
and zero-shot, and achieves new state-of-the-art results.
